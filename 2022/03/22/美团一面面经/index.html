<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="美团一面面经"><meta name="keywords" content="深度学习"><meta name="author" content="Mello13"><meta name="copyright" content="Mello13"><title>美团一面面经 | mello's blog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.1"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css?version=1.9.1"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://hm.baidu.com"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?d5cba8acec7c962be4afb068976dd3fc";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  hexoVersion: '5.4.2'
} </script><meta name="generator" content="Hexo 5.4.2"></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="false"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Bert%E7%9A%84%E7%BB%93%E6%9E%84"><span class="toc-number">1.</span> <span class="toc-text">Bert的结构</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81%E6%95%B4%E4%B8%AA%E7%BB%93%E6%9E%84%E7%9A%84%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.1.</span> <span class="toc-text">1、整个结构的介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81encoder%E7%BB%93%E6%9E%84%E7%9A%84%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.2.</span> <span class="toc-text">2、encoder结构的介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81%E8%BE%93%E5%85%A5%E7%9A%84%E4%BD%8D%E7%BD%AE%E5%90%91%E9%87%8F%E6%98%AF%E6%80%8E%E4%B9%88%E5%BE%97%E5%88%B0%E7%9A%84"><span class="toc-number">1.3.</span> <span class="toc-text">3、输入的位置向量是怎么得到的</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4%E3%80%81%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E7%94%A8multi-attention%EF%BC%9F"><span class="toc-number">1.4.</span> <span class="toc-text">4、为什么要用multi-attention？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5%E3%80%81BERT%E7%9A%84%E5%A4%9A%E5%A4%B4%E6%98%AF%E6%80%8E%E4%B9%88%E5%88%86%E5%89%B2%E7%9A%84%EF%BC%9F"><span class="toc-number">1.5.</span> <span class="toc-text">5、BERT的多头是怎么分割的？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Transformer%E7%9A%84%E7%BB%93%E6%9E%84"><span class="toc-number">2.</span> <span class="toc-text">Transformer的结构</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Attention%E6%9C%BA%E5%88%B6"><span class="toc-number">3.</span> <span class="toc-text">Attention机制</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97"><span class="toc-number">4.</span> <span class="toc-text">随机森林</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E5%BA%8F%E5%88%97-%E4%BA%8C"><span class="toc-number">5.</span> <span class="toc-text">最长公共子序列(二)</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">Mello13</div><div class="author-info__description text-center">当未来没有来时，它还没有来</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">40</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">24</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">4</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://xyzinc-xyx.github.io/">XYZinc</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://raw.githubusercontent.com/SkylerSuen/PicBase/master/topimg.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">mello's blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"></span></div><div id="post-info"><div id="post-title">美团一面面经</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2022-03-22</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E5%88%B7%E7%BB%8F%E9%AA%8C/">刷经验</a></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="Bert的结构"><a href="#Bert的结构" class="headerlink" title="Bert的结构"></a>Bert的结构</h1><h2 id="1、整个结构的介绍"><a href="#1、整个结构的介绍" class="headerlink" title="1、整个结构的介绍"></a>1、整个结构的介绍</h2><p>（1）模型输入</p>
<pre><code>   输入句子分字符，字符的embedding+position embedding + segement embedding求和作为模型的输入，其中会随机掩盖掉15%的字符，其中80%用[mask]字符代替，这10%的随机替换成其它字符，10%不做改变。
</code></pre>
<p>（2）模型输出：</p>
<pre><code> 预训练模型的输出是输入的各个字符经过模型编码后的embedding，两个任务是预测输入中被掩盖掉的词，以及预测输入的两句是上下两句话还是拼接的。
</code></pre>
<p>（3）模型结构：</p>
<pre><code>   12层的transformer中的encoder组成，每层encoder由4个部分组成，首先是一个multi-attention，后面结一层残差计算加layer normalization，再加上一层前向全连接层，通过relu激活，增加非线性性(多个头的拼接是线性的)，最后就是一次残差计算加layer normalization。
</code></pre>
<h2 id="2、encoder结构的介绍"><a href="#2、encoder结构的介绍" class="headerlink" title="2、encoder结构的介绍"></a>2、encoder结构的介绍</h2><pre><code>   multi-attention后面接一层残差网络，然后再做一个layer normalization

   feed forward后面接一层残差网络，然后再做一个layer normalization
</code></pre>
<h2 id="3、输入的位置向量是怎么得到的"><a href="#3、输入的位置向量是怎么得到的" class="headerlink" title="3、输入的位置向量是怎么得到的"></a>3、输入的位置向量是怎么得到的</h2><pre><code>  在BERT中使用的是随机初始化一个向量，然后用数据学习这个位置向量。在transformer中是使用三角函数计算的。两者的计算方式和优缺点可以参考：自注意力与位置编码
</code></pre>
<h2 id="4、为什么要用multi-attention？"><a href="#4、为什么要用multi-attention？" class="headerlink" title="4、为什么要用multi-attention？"></a>4、为什么要用multi-attention？</h2><pre><code>   Attention： Attention机制的中文名叫“注意力机制”，顾名思义，它的主要作用是让神经网络把“注意力”放在一部分输入上，即：区分输入的不同部分对输出的影响。

   Self-Attention旨在用文本中的其它字来增强目标字的语义表示。由于每个注意力头初始化参数不一样，所以每个注意力头可以学习到不同的注意力空间。在不同的语义场景下，Attention所重点关注的字应有所不同。因此，Multi-head Self-Attention可以理解为考虑多种语义场景下目标字与文本中其它字的语义向量的不同融合方式。
</code></pre>
<h2 id="5、BERT的多头是怎么分割的？"><a href="#5、BERT的多头是怎么分割的？" class="headerlink" title="5、BERT的多头是怎么分割的？"></a>5、BERT的多头是怎么分割的？</h2><pre><code>    BERT在每个attention的计算中，是直接用输入的词向量分别跟三个768*768大小的矩阵相乘获得512*768的query，key和value矩阵的，然后每个矩阵在768的那个维度上进行切割，切割成12个512*64的矩阵，那么每个头的query、key和value都是512*64的矩阵，每一层有12个这样的头。这就是多头的切割。
</code></pre>
<h1 id="Transformer的结构"><a href="#Transformer的结构" class="headerlink" title="Transformer的结构"></a>Transformer的结构</h1><p>Transformer主要由多头self-Attenion和Feed Forward Neural Network组成。Transformer的可训练的神经网络可以通过堆叠Transformer的形式进行搭建，作者的实验是通过搭建编码器和解码器各6层，总共12层的Encoder-Decoder，并在机器翻译中取得了BLEU值得新高。是一个典型的 encoder-decoder 模型。在 self-attention 中，Q=K=V，序列中的每个单词(token)和该序列中其余单词(token)进行 attention 计算。self-attention 的特点在于「无视词(token)之间的距离直接计算依赖关系，从而能够学习到序列的内部结构」。</p>
<h1 id="Attention机制"><a href="#Attention机制" class="headerlink" title="Attention机制"></a>Attention机制</h1><p>encoder提供了更多的数据给到decoder，encoder会把所有的节点的hidden state提供给decoder，而不仅仅只是encoder最后一个节点的hidden state。decoder并不是直接把所有encoder提供的hidden state作为输入，而是采取一种选择机制，把最符合当前位置的hidden state选出来，具体的步骤如下：</p>
<p>1、计算每一个hidden state的分数值；</p>
<p>2、对每个分数值做一个softmax的计算，这能让相关性高的hidden state的分数值更大，相关性低的hidden state的分数值更低；</p>
<p>3、加权求和。</p>
<p>Attention函数的本质可以被描述为一个查询（query）到一系列键key-值value对的映射：</p>
<p>$ Attention(Query,Score)=\sum(Similarity(Query,Key)*Value) $</p>
<p><a target="_blank" rel="noopener" href="http://t.csdn.cn/dLBqo">http://t.csdn.cn/dLBqo</a></p>
<h1 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h1><h1 id="最长公共子序列-二"><a href="#最长公共子序列-二" class="headerlink" title="最长公共子序列(二)"></a>最长公共子序列(二)</h1></div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Mello13</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://skylersuen.github.io/2022/03/22/美团一面面经/">https://skylersuen.github.io/2022/03/22/美团一面面经/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2022/08/27/%E3%80%90C++%E3%80%91%E5%9B%BE/"><i class="fa fa-chevron-left">  </i><span>【C++】图</span></a></div><div class="next-post pull-right"><a href="/2022/03/22/%E9%98%BF%E9%87%8C%E4%B8%80%E9%9D%A2/"><span>阿里一面面经</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://raw.githubusercontent.com/SkylerSuen/PicBase/master/topimg.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2022 - 2024 By Mello13</div><div class="framework-info"><span>Driven - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">if you have any suggestion, please contact me at syf_mello@163.com !</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/lib/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.1"></script><script src="/js/fancybox.js?version=1.9.1"></script><script src="/js/sidebar.js?version=1.9.1"></script><script src="/js/copy.js?version=1.9.1"></script><script src="/js/fireworks.js?version=1.9.1"></script><script src="/js/transition.js?version=1.9.1"></script><script src="/js/scroll.js?version=1.9.1"></script><script src="/js/head.js?version=1.9.1"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script></body></html>
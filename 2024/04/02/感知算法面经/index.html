<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="感知算法面经"><meta name="keywords" content="深度学习"><meta name="author" content="Mello13"><meta name="copyright" content="Mello13"><title>感知算法面经 | mello's blog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.1"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css?version=1.9.1"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://hm.baidu.com"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?d5cba8acec7c962be4afb068976dd3fc";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  hexoVersion: '5.4.2'
} </script><meta name="generator" content="Hexo 5.4.2"></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="false"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA"><span class="toc-number">1.</span> <span class="toc-text">输入输出</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81"><span class="toc-number">2.</span> <span class="toc-text">代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%AF%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8F%82%E6%95%B0"><span class="toc-number">3.</span> <span class="toc-text">可学习的参数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#nms"><span class="toc-number"></span> <span class="toc-text">NMS</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%98%BE%E5%AD%98%E8%BF%87%E9%AB%98"><span class="toc-number"></span> <span class="toc-text">显存过高</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number"></span> <span class="toc-text">如何解决过拟合</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E5%92%8C%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1"><span class="toc-number"></span> <span class="toc-text">梯度爆炸和梯度消失</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95"><span class="toc-number">1.</span> <span class="toc-text">解决方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%96%B9%E9%9D%A2"><span class="toc-number">1.1.</span> <span class="toc-text">模型方面：</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#mse%E5%92%8Cce%E5%88%86%E7%B1%BB%E4%B8%BA%E4%BB%80%E4%B9%88%E7%94%A8ce%E8%80%8C%E4%B8%8D%E6%98%AFmse"><span class="toc-number"></span> <span class="toc-text">MSE和CE：分类为什么用CE而不是MSE</span></a></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">Mello13</div><div class="author-info__description text-center">当未来没有来时，它还没有来</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">51</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">24</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">5</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://xyzinc-xyx.github.io/">XYZinc</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://raw.githubusercontent.com/SkylerSuen/PicBase/master/topimg.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">mello's blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"></span></div><div id="post-info"><div id="post-title">感知算法面经</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2024-04-02</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E5%88%B7%E7%BB%8F%E9%AA%8C/">刷经验</a></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>https://zhuanlan.zhihu.com/p/642370818 ## BN</p>
<h3 id="输入输出">输入输出</h3>
<p>批量归一化（Batch
Normalization，BN）层是神经网络中常用的一种层，它可以缩放和平移每个输入通道的值，使得输出的均值接近0，方差接近1。这样可以稳定神经网络的训练，加速收敛，同时也有一定的正则化效果。</p>
<p>给定输入的维度是(N, C, H, W)，其中N是批量大小（batch
size），C是通道数（channels），H是高度（height），W是宽度（width）。</p>
<p>BN层的输出的维度应该与输入的维度相同，也是(N, C, H,
W)。这是因为BN层对每个通道独立进行归一化，不改变通道数，也不改变每个通道的空间尺寸。</p>
<p>具体地，BN层首先计算每个通道的均值和方差，然后用每个通道的值减去均值并除以标准差，得到归一化的值。然后，BN层再用学习到的缩放参数和平移参数对归一化的值进行缩放和平移，得到最终的输出。这些操作都是逐元素进行的，所以不会改变输入的维度。</p>
<p>需要注意的是，以上是在训练阶段的操作。在推理阶段，BN层通常使用移动平均和移动方差代替每个批次的均值和方差。
### 作用 Batch
Normalization（BN）层的主要作用是通过正则化层的输入以减少"Internal
Covariate
Shift内部协变量偏移"，从而使得深度网络的训练变得更稳定。Internal
Covariate
Shift是指训练神经网络时由于每一层参数的变化导致后一层输入分布的变化，这种变化会导致训练过程中每一层都需要不断地适应新的数据分布，使训练过程变得复杂并且可能导致训练过程陷入饱和区，从而影响训练速度。</p>
<p>BN层的引入，使得每一层的输入都近似标准正态分布，这样一来，后一层的输入分布就不会随着前一层参数的变化而变化，从而缓解了Internal
Covariate Shift的问题。这样，BN层可以：</p>
<ol type="1">
<li>加速神经网络的收敛速度。</li>
<li>降低模型对初始化的敏感度。</li>
<li>具有一定的正则化效果，减少模型的过拟合。</li>
</ol>
<h3 id="代码">代码</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">BN</span>(<span class="params">inputs</span>):</span><br><span class="line">    c = inputs.shape[<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># print(c)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(c):</span><br><span class="line">        channel = inputs[:,i,:,:]</span><br><span class="line">        mean = torch.Tensor.mean(channel)</span><br><span class="line">        var = torch.Tensor.var(channel,<span class="literal">False</span>)</span><br><span class="line">        channel_new = ((channel - mean)  /   (torch.<span class="built_in">pow</span>(var + <span class="number">1e-5</span>,<span class="number">0.5</span>))  )  * <span class="number">1</span> + <span class="number">0</span></span><br><span class="line">        inputs[:,i,:,:] = channel_new</span><br><span class="line">    <span class="keyword">return</span> inputs</span><br><span class="line">torch_bn = nn.BatchNorm2d(<span class="number">3</span>)</span><br><span class="line">x = (torch.randn(<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">2</span>)) *<span class="number">10</span></span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line">y = BN(x.detach())</span><br><span class="line">z = torch_bn(x)</span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="built_in">print</span>(z)</span><br></pre></td></tr></table></figure>
<h3 id="可学习的参数">可学习的参数</h3>
<p>在BN层中，每个特征通道有两个可学习的参数，一个是缩放参数γ（gamma），一个是平移参数β（beta）。这两个参数是在对输入数据做标准化处理之后引入的，目的是保持模型的表达能力。如果在BN操作后，模型需要原始的、未归一化的特征，那么这可以通过学习合适的γ和β参数来实现。</p>
<h2 id="nms">NMS</h2>
<ol type="1">
<li>选取这类box中scores最大的哪一个，它的index记为 i ，并保留它；</li>
<li>计算 boxes[i] 与其余的 boxes 的 IOU 值；</li>
<li>如果其 IOU&gt;0.5
了，那么就舍弃这个box（由于可能这两个box表示同一目标，所以保留分数高的哪一个）；</li>
<li>从最后剩余的boxes中，再找出最大scores的哪一个，如此循环往复。
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">nms</span>(<span class="params">boxes, scores, threshold=<span class="number">0.5</span>, top_k=<span class="number">200</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        boxes: 预测出的box, shape[M,4]</span></span><br><span class="line"><span class="string">        scores: 预测出的置信度，shape[M]</span></span><br><span class="line"><span class="string">        threshold: 阈值</span></span><br><span class="line"><span class="string">        top_k: 要考虑的box的最大个数</span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">        keep: nms筛选后的box的新的index数组</span></span><br><span class="line"><span class="string">        count: 保留下来box的个数</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    keep = scores.new(scores.size(<span class="number">0</span>)).zero_().long()</span><br><span class="line">    x1 = boxes[:, <span class="number">0</span>]</span><br><span class="line">    y1 = boxes[:, <span class="number">1</span>]</span><br><span class="line">    x2 = boxes[:, <span class="number">2</span>]</span><br><span class="line">    y2 = boxes[:, <span class="number">3</span>]</span><br><span class="line"> </span><br><span class="line">    area = (x2-x1)*(y2-y1)  <span class="comment"># 面积,shape[M]</span></span><br><span class="line">    _, idx = scores.sort(<span class="number">0</span>, descending=<span class="literal">True</span>) <span class="comment"># 降序排列scores的值大小</span></span><br><span class="line">    <span class="comment"># 取前top_k个进行nms</span></span><br><span class="line">    idx = idx[:top_k]</span><br><span class="line"> </span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line"> </span><br><span class="line">    <span class="keyword">while</span> idx.numel():</span><br><span class="line">        <span class="comment"># 记录最大score值的index</span></span><br><span class="line">        i = idx[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 保存到keep中</span></span><br><span class="line">        keep[count] = i</span><br><span class="line">        <span class="comment"># keep 的序号</span></span><br><span class="line">        count += <span class="number">1</span></span><br><span class="line"> </span><br><span class="line">        <span class="keyword">if</span> idx.size(<span class="number">0</span>) == <span class="number">1</span>: <span class="comment"># 保留框只剩一个</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"> </span><br><span class="line">        idx = idx[<span class="number">1</span>:] <span class="comment"># 移除已经保存的index</span></span><br><span class="line"> </span><br><span class="line">        <span class="comment"># 计算boxes[i]和其他boxes之间的iou</span></span><br><span class="line">        xx1 = x1[idx].clamp(<span class="built_in">min</span>=x1[i])</span><br><span class="line">        yy1 = y1[idx].clamp(<span class="built_in">min</span>=y1[i])</span><br><span class="line">        xx2 = x2[idx].clamp(<span class="built_in">max</span>=x2[i])</span><br><span class="line">        yy2 = y2[idx].clamp(<span class="built_in">max</span>=y2[i])</span><br><span class="line"> </span><br><span class="line">        w = (xx2 - xx1).clamp(<span class="built_in">min</span>=<span class="number">0</span>)</span><br><span class="line">        h = (yy2 - yy1).clamp(<span class="built_in">min</span>=<span class="number">0</span>)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># 交集的面积</span></span><br><span class="line">        inter = w * h  <span class="comment"># shape[M-1]</span></span><br><span class="line">        iou = inter / (area[i] + area[idx] - inter)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># iou满足条件的idx</span></span><br><span class="line">        idx = idx[iou.le(threshold)] <span class="comment"># Shape[M-1]</span></span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> keep, count</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="显存过高">显存过高</h2>
<p>在训练模型时，如果显存占用很高但GPU利用率不高，这可能由以下原因导致：</p>
<p>CPU和GPU之间的数据传输瓶颈：
在训练过程中，数据需要从CPU传输到GPU。如果这个传输过程过慢，就可能导致GPU在等待数据时处于空闲状态，从而降低其利用率。</p>
<p>训练批量（Batch size）过小：
如果训练批量过小，GPU可能无法充分利用其并行计算能力，导致利用率低。</p>
<p>模型计算复杂度低：
如果模型的计算复杂度低（比如模型较小，或者模型的运算并不复杂），那么GPU的计算能力可能没有被充分利用。</p>
<p>同步操作： 一些同步操作，如同步Batch
Normalization或数据加载等，也可能导致GPU等待，降低其利用率。</p>
<p>IO瓶颈：
如果数据从磁盘读取到内存的速度跟不上模型的训练速度，也会使GPU在等待新的数据时空闲，从而降低利用率。</p>
<p>显存不足：
显存不足会限制网络的大小和批量大小，甚至可能导致无法运行网络，也会导致GPU利用率低。</p>
<p>解决上述问题的方法包括：优化数据传输过程、增大训练批量（前提是显存允许）、选择更复杂的模型、优化同步操作、提升IO速度、升级GPU或降低模型和批量的大小等。</p>
<h2 id="如何解决过拟合">如何解决过拟合</h2>
<ol type="1">
<li>正则化 2. Early Stopping 3. Dropout 4. 数据增强 5.减小模型复杂度 ###
正则化方法L0L1L2 正则化是加在loss function后面的</li>
</ol>
<p>L0范数是指向量中非0的元素的个数。</p>
<p>L1范数是指向量中各个元素绝对值之和，也叫“稀疏规则算子”（Lasso
regularization）。</p>
<p>L0L1两者都可以实现稀疏性，既然L0可以实现稀疏，为什么不用L0，而要用L1呢？个人理解一是因为L0范数很难优化求解（NP难问题），二是L1范数是L0范数的最优凸近似，而且它比L0范数要容易优化求解。所以大家才把目光和万千宠爱转于L1范数。</p>
<p>L2范数是指向量各元素的平方和然后求平方根。可以使得W的每个元素都很小，都接近于0。</p>
<p>但L2与L1范数不同，它不会让它等于0，而是接近于0。L2正则项起到使得参数w变小加剧的效果，但是为什么可以防止过拟合呢？一个通俗的理解便是：更小的参数值w意味着模型的复杂度更低，对训练数据的拟合刚刚好（奥卡姆剃刀），不会过分拟合训练数据，从而使得不会过拟合，以提高模型的泛化能力。还有就是看到有人说L2范数有助于处理
condition
number不好的情况下矩阵求逆很困难的问题（具体这儿我也不是太理解）。</p>
<p>L1使权重稀疏，L2使权重平滑。L1优点是能够获得sparse模型，对于large-scale的问题来说这一点很重要，因为可以减少存储空间；AL2优点是实现简单，能够起到正则化的作用。缺点就是L1的优点：无法获得sparse模型。
### numpy实现L1(广播机制)
在符合广播条件的前提下，广播机制会为尺寸较小的向量添加一个轴（广播轴），使其维度信息与较大向量的相同。</p>
<p>计算 m2 的矩阵与 n * 2 的矩阵中，m2 的每一行到 n*2
的两两之间欧氏距离。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># L2 = sqrt((x1-x2)^2 + (y1-y2)^2 + (z1-z2)^2)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">L2_dist_1</span>(<span class="params">cloud1, cloud2</span>):</span><br><span class="line">    m, n = <span class="built_in">len</span>(cloud1), <span class="built_in">len</span>(cloud2)</span><br><span class="line">    <span class="comment"># project 01</span></span><br><span class="line">    <span class="comment"># cloud1 = np.repeat(cloud1, n, axis=0) # (n*m,2)</span></span><br><span class="line">    <span class="comment"># cloud1 = np.reshape(cloud1, (m, n, -1)) # (m,n,2) (n,2)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># project 02</span></span><br><span class="line">    <span class="comment"># cloud1 = cloud1[:, None, :] # (m,1,2)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># project 03</span></span><br><span class="line">    cloud1 = np.expand_dims(cloud1, <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    dist = np.sqrt(np.<span class="built_in">sum</span>((cloud1 - cloud2)**<span class="number">2</span>, axis=<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> dist</span><br></pre></td></tr></table></figure>
<h2 id="梯度爆炸和梯度消失">梯度爆炸和梯度消失</h2>
<p>由于链式法则</p>
<p>梯度爆炸：在深度神经网络中，如果每一层的权重都较大，那么在反向传播过程中，梯度会随着层数的增加而指数级增大，这就是梯度爆炸。梯度爆炸会导致模型权重更新过大，使得模型无法收敛，甚至可能导致数值溢出。</p>
<p>梯度消失：相反，如果每一层的权重都较小，那么在反向传播过程中，梯度会随着层数的增加而指数级减小，这就是梯度消失。梯度消失会导致模型权重更新过小，使得模型学习速度变慢，甚至无法学习。</p>
<h3 id="解决方法">解决方法</h3>
<p>怎么解决梯度爆炸和梯度消失</p>
<h4 id="模型方面">模型方面：</h4>
<ol type="1">
<li><p>梯度裁剪（Gradient
Clipping）：这是一种常用的防止梯度爆炸的技术，它通过设置一个阈值，当梯度的模大于这个阈值时，将梯度缩放到这个阈值，从而防止梯度过大。</p></li>
<li><p>权重初始化：合适的权重初始化可以在一定程度上缓解梯度消失和梯度爆炸问题。例如，Xavier初始化和He初始化就是两种常用的权重初始化方法。</p></li>
<li><p>使用ReLU及其变种：ReLU激活函数及其变种（如Leaky
ReLU、PReLU等）可以在一定程度上缓解梯度消失问题，因为它们在正数区间的梯度为常数。</p></li>
<li><p>批量归一化（Batch
Normalization）：批量归一化可以使得每一层的输入都有相同的分布，从而缓解梯度消失和梯度爆炸问题。</p></li>
<li><p>残差连接restnet（Residual
Connection）：在深度神经网络中添加残差连接可以使得梯度直接通过残差连接反向传播，从而缓解梯度消失问题。
#### 数据方面：</p></li>
<li><p>数据标准化：
通过将输入数据标准化到一个合适的尺度（例如，使其具有0均值和1标准差），可以防止数据的尺度过大导致的梯度爆炸。</p></li>
<li><p>避免使用过大的学习率：
使用过大的学习率可能会导致梯度爆炸。使用学习率衰减策略，或者使用自适应学习率优化器（如Adam、Adagrad等）可以避免这个问题。</p></li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">nms</span>(<span class="params">boxes,scores,threshold</span>):</span><br><span class="line">    x1 = [: , <span class="number">0</span>]</span><br><span class="line">    x2 = [: , <span class="number">1</span>]</span><br><span class="line">    y1 = [: , <span class="number">2</span>]</span><br><span class="line">    y2 = [: , <span class="number">3</span>]</span><br><span class="line">    areas = (x2-x1)*(y2-y1)</span><br><span class="line">    keep  = []</span><br><span class="line"></span><br><span class="line">    _,order = scores.sort(<span class="number">0</span>,descending=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">while</span> order.numel()&gt;<span class="number">0</span>:</span><br><span class="line">        i = order[<span class="number">0</span>]</span><br><span class="line">        keep.append(i)</span><br><span class="line">        <span class="keyword">if</span> order.numel()==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        xx1 = x1[idx].clamp(<span class="built_in">min</span>=x1[i])</span><br><span class="line">        xx2 = x2[idx].clamp(<span class="built_in">max</span>=x2[i])</span><br><span class="line">        yy1 = y1[idx].clamp(<span class="built_in">min</span>=y1[i])</span><br><span class="line">        yy2 = y2[idx].clamp(<span class="built_in">max</span>=y2[i])</span><br><span class="line">        w = (xx2 - xx1).clamp(<span class="built_in">min</span>=<span class="number">0</span>)</span><br><span class="line">        h = (yy2 - yy1).clamp(<span class="built_in">min</span>=<span class="number">0</span>)</span><br><span class="line">        inter = w * h  <span class="comment"># shape[M-1]</span></span><br><span class="line">        iou = inter / (area[i] + area[idx] - inter)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># iou满足条件的idx</span></span><br><span class="line">        scores = scores[iou.le(threshold)] <span class="comment"># Shape[M-1]</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="mse和ce分类为什么用ce而不是mse">MSE和CE：分类为什么用CE而不是MSE</h2>
<p>MSE在logistic 回归中不是一个凸函数，而交叉熵在logistic
回归中是一个凸函数。
而在分类问题中，我们通常会用梯度下降手段通过最小化损失函数去寻找系数的最优解。因此如果损失函数是非凸的，我们无法保证总能找到全局最小值，甚至可能会陷入局部最小值。所以选交叉熵而不是MSE</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Mello13</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://skylersuen.github.io/2024/04/02/感知算法面经/">https://skylersuen.github.io/2024/04/02/感知算法面经/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2024/04/04/%E5%A4%9A%E6%A8%A1%E6%80%81%E9%9D%A2%E7%BB%8F/"><i class="fa fa-chevron-left">  </i><span>多模态面经</span></a></div><div class="next-post pull-right"><a href="/2024/04/01/tiktok%E4%B8%80%E9%9D%A2%E5%9B%9E%E5%BF%86/"><span>tiktok一面回忆</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://raw.githubusercontent.com/SkylerSuen/PicBase/master/topimg.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2022 - 2024 By Mello13</div><div class="framework-info"><span>Driven - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">if you have any suggestion, please contact me at syf_mello@163.com !</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/lib/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.1"></script><script src="/js/fancybox.js?version=1.9.1"></script><script src="/js/sidebar.js?version=1.9.1"></script><script src="/js/copy.js?version=1.9.1"></script><script src="/js/fireworks.js?version=1.9.1"></script><script src="/js/transition.js?version=1.9.1"></script><script src="/js/scroll.js?version=1.9.1"></script><script src="/js/head.js?version=1.9.1"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script></body></html>